{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE_URL = 'https://www.rusprofile.ru/codes/610000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    \"\"\"Makes tasty soup out of raw HTML\"\"\"\n",
    "    return soup(requests.get(url).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Number of Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape from: 274\n"
     ]
    }
   ],
   "source": [
    "base_page_soup = get_soup(PAGE_URL) # get page soup to extract n of pages\n",
    "pg_list = base_page_soup.find('ul', class_=\"paging-list\") # find pagination item on the page\n",
    "N_PAGES = int(pg_list.find_all('li')[-2].text)\n",
    "iterator_arg = range(1, N_PAGES + 1) # define iterator for browsing pages\n",
    "print(f'Total pages to scrape from: {N_PAGES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scapping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_companies(page_soup):\n",
    "    \"\"\"Finds companies on the page. Returns tags list\"\"\"\n",
    "    return page_soup.find_all('div', class_ = 'company-item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_dict = {\n",
    "    'января' : 1, 'февраля' : 2, 'марта' : 3, 'апреля' : 4, 'мая' : 5, 'июня' : 6,\n",
    "    'июля' : 7, 'августа' : 8, 'сентября' : 9, 'октября' : 10, 'ноября' : 11, 'декабря' : 12\n",
    "} # dictionary for transferring russian names into integer months numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_company(company):\n",
    "    \"\"\"Extracts data from each company item\"\"\"\n",
    "    company_name = company.find('div', class_=\"company-item__title\").find('a').text.strip()\n",
    "    try:\n",
    "        company_status = company.find('span', class_=\"warning-text\").text.strip()\n",
    "    except:\n",
    "        company_status = 'Организация работает'\n",
    "    company_address = ', '.join(company.find('address',  class_=\"company-item__text\").text.strip().split(', ')[1:4])\n",
    "    try:\n",
    "        lst = company.findAll('div', class_='company-item-info')[1].findAll('dd')\n",
    "    except:\n",
    "        lst = company.find('div',  class_=\"company-item-info\").findAll('dd')\n",
    "    find_date = [i.text for i in lst if 'г.' in i.text]\n",
    "    find_cap = [i.text for i in lst if 'руб.' in i.text]\n",
    "    if len(find_date) != 0:\n",
    "        company_estdate = find_date[0].strip(' г.').split(' ')\n",
    "        company_estdate = pd.Timestamp(f'{company_estdate[2]}-{month_dict[company_estdate[1]]}-{company_estdate[0]}')\n",
    "    else:\n",
    "        company_estdate = None\n",
    "        \n",
    "    if len(find_cap) != 0:\n",
    "        company_cap = pd.to_numeric(find_cap[0].strip(' руб.').replace(' ', '').replace(',', '.'))\n",
    "    else:\n",
    "        company_cap = None\n",
    "    company_activity = company.findAll('div',  class_=\"company-item-info\")[-1].find('dd').text.strip()\n",
    "    return pd.Series({'name' : company_name, 'status' : company_status,\n",
    "                      'address' : company_address, 'est_date' : company_estdate,\n",
    "                      'cap' : company_cap, 'activity' : company_activity})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url_iterable = PAGE_URL + '/{}/' # Making the string changable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_num = PAGE_URL.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLEEP_TIME = 10 # Essential! Could be increased manually if needed. Defines pause betweeen opening the pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b5d4e49ea34f3bbba36d78ad7616e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=917), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-f213fb344873>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpage_n\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator_arg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcurrent_page_soup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_url_iterable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSLEEP_TIME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mcompanies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_companies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_page_soup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcurrent_page_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscrape_company\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcompany\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcompanies\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "for page_n in tqdm(iterator_arg):\n",
    "    current_page_soup = get_soup(page_url_iterable.format(page_n))\n",
    "    sleep(SLEEP_TIME)\n",
    "    companies = get_companies(current_page_soup)\n",
    "    current_page_df = pd.DataFrame([scrape_company(company) for company in companies])\n",
    "    df_list.append(current_page_df)\n",
    "    \n",
    "full_cat_df = pd.concat(df_list)\n",
    "full_cat_df = full_cat_df.assign(base_category = int(cat_num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emergency cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use in case of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(df_list).assign(base_category = int(cat_num)).to_csv(f'data/database_{cat_num}_part_2.csv', sep = ';', index = None, encoding = 'utf-8')\n",
    "iterator_arg = range(page_n, N_PAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell is set to be used after inital scrapping result is ready to concatenate with previously scrapped data before the error occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cat_df = pd.concat([\n",
    "    full_cat_df,\n",
    "    pd.read_csv(f'data/database_{cat_num}_part.csv').assign(base_category = int(cat_num))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cat_df = full_cat_df[full_cat_df.activity.str.contains(cat_num[:2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cat_df.to_csv(f'data/database_{cat_num}.csv', index = None, encoding = 'utf-8', sep = ';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
